#!/usr/bin/env python3
"""
Script per aggiungere documentazione da URL a Qdrant con semantic chunking.
Legge URL da un file (una per riga) e li indicizza usando Chonkie + semchunk.

Pipeline:
1. Download HTML
2. Extract text
3. Clean text (unicode, whitespace, boilerplate)
4. Semantic chunking (Chonkie â†’ macro-blocks)
5. Fine chunking (semchunk â†’ embedding-ready chunks)
6. Embedding (Ollama nomic-embed-text)
7. Upload to Qdrant

Usage:
    python3 add_urls_to_qdrant.py <file_urls.txt> [--chunk-size 500] [--verbose]
"""

import sys
import logging
import argparse
from pathlib import Path

# Import local library modules
from lib import (
    clean_text,
    validate_text_quality,
    semantic_chunk_text,
    fine_chunk_text,
    filter_chunks,
    batch_embed_chunks,
    batch_upload_chunks,
    check_qdrant_connection,
    ChunkingError,
)

# tqdm for progress bars
from tqdm import tqdm

# BeautifulSoup for HTML parsing
import requests
from bs4 import BeautifulSoup


# Configuration
DEFAULT_CHUNK_SIZE = 500  # tokens
DEFAULT_OVERLAP = 50  # tokens
MAX_TOKENS = 8192  # nomic-embed-text limit


def setup_logging(verbose: bool = False):
    """Configure logging"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%H:%M:%S'
    )


def fetch_url_content(url: str) -> tuple[str, str] | tuple[None, None]:
    """
    Download and extract text from URL.
    
    Args:
        url: URL to fetch
        
    Returns:
        (text, title) tuple or (None, None) if failed
    """
    try:
        logging.info(f"ðŸ“¥ Downloading: {url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove unwanted elements
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe']):
            element.decompose()
        
        # Extract text
        text = soup.get_text(separator='\n', strip=True)
        
        # Extract title
        title = soup.find('title')
        title_text = title.get_text().strip() if title else url
        
        logging.info(f"ðŸ“„ Title: {title_text}")
        logging.info(f"ðŸ“ Raw length: {len(text)} characters")
        
        return text, title_text
        
    except Exception as e:
        logging.error(f"âŒ Download failed: {e}")
        return None, None


def process_url(
    url: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    overlap: int = DEFAULT_OVERLAP,
    strict: bool = False
) -> dict:
    """
    Process a single URL through the complete pipeline.
    
    Args:
        url: URL to process
        chunk_size: Target chunk size in tokens
        overlap: Overlap between chunks in tokens
        
    Returns:
        Statistics dictionary
    """
    stats = {
        'url': url,
        'success': False,
        'raw_chars': 0,
        'clean_chars': 0,
        'macro_chunks': 0,
        'final_chunks': 0,
        'uploaded_chunks': 0,
        'errors': []
    }
    
    # 1. Download
    raw_text, title = fetch_url_content(url)
    if raw_text is None:
        stats['errors'].append('Download failed')
        return stats
    
    stats['raw_chars'] = len(raw_text)
    
    # 2. Clean text
    logging.info("ðŸ§¹ Cleaning text...")
    cleaned_text = clean_text(raw_text)
    stats['clean_chars'] = len(cleaned_text)
    
    # Validate quality
    if not validate_text_quality(cleaned_text, min_length=100):
        logging.warning("âš ï¸  Text quality too low, skipping")
        stats['errors'].append('Text quality too low')
        return stats
    
    logging.info(f"âœ“ Cleaned: {stats['clean_chars']} chars ({stats['raw_chars'] - stats['clean_chars']} removed)")
    
    # 3. Semantic chunking (Chonkie)
    logging.info("âœ‚ï¸  Semantic chunking (Chonkie)...")
    try:
        macro_chunks = semantic_chunk_text(cleaned_text, chunk_size=chunk_size * 2, chunk_overlap=overlap)
        stats['macro_chunks'] = len(macro_chunks)
        logging.info(f"âœ“ Created {stats['macro_chunks']} macro-semantic blocks")
    except ChunkingError as e:
        if strict:
            raise
        logging.warning(f"Chunking failed: {e}, skipping URL")
        stats['errors'].append(f'Chunking error: {e}')
        return stats
    
    # 4. Fine chunking (semchunk)
    logging.info("ðŸ”¬ Fine chunking (semchunk)...")
    try:
        fine_chunks = fine_chunk_text(macro_chunks, target_tokens=chunk_size, overlap_tokens=overlap)
        logging.info(f"âœ“ Created {len(fine_chunks)} fine chunks")
    except ChunkingError as e:
        if strict:
            raise
        logging.warning(f"Fine chunking failed: {e}, skipping URL")
        stats['errors'].append(f'Fine chunking error: {e}')
        return stats
    
    # 5. Filter chunks
    logging.info("ðŸ” Filtering chunks...")
    valid_chunks = filter_chunks(fine_chunks, min_tokens=50, max_tokens=MAX_TOKENS)
    stats['final_chunks'] = len(valid_chunks)
    
    if stats['final_chunks'] == 0:
        logging.warning("âš ï¸  No valid chunks after filtering")
        stats['errors'].append('No valid chunks')
        return stats
    
    # Calculate statistics
    token_counts = [c['token_count'] for c in valid_chunks]
    avg_tokens = sum(token_counts) / len(token_counts)
    min_tokens = min(token_counts)
    max_tokens_actual = max(token_counts)
    
    logging.info(f"ðŸ“Š Chunk stats: avg={avg_tokens:.0f}, min={min_tokens}, max={max_tokens_actual} tokens")
    
    # 6. Embedding
    logging.info(f"ðŸ¤– Generating embeddings ({len(valid_chunks)} chunks)...")
    embedded_chunks = batch_embed_chunks(valid_chunks, max_tokens=MAX_TOKENS)
    
    if len(embedded_chunks) == 0:
        logging.error("âŒ All embeddings failed")
        stats['errors'].append('Embedding failed')
        return stats
    
    logging.info(f"âœ“ Embedded {len(embedded_chunks)} chunks")
    
    # 7. Upload to Qdrant
    logging.info("ðŸ“¤ Uploading to Qdrant...")
    uploaded = batch_upload_chunks(embedded_chunks, url, title)
    stats['uploaded_chunks'] = uploaded
    
    if uploaded > 0:
        stats['success'] = True
        logging.info(f"âœ… Successfully uploaded {uploaded} chunks for: {title}")
    else:
        logging.error("âŒ Upload failed")
        stats['errors'].append('Upload failed')
    
    return stats


def process_urls_file(
    filename: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    overlap: int = DEFAULT_OVERLAP,
    strict: bool = False
):
    """
    Process all URLs from a file.
    
    Args:
        filename: Path to file with URLs (one per line)
        chunk_size: Target chunk size in tokens
        overlap: Overlap between chunks in tokens
    """
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        logging.info(f"ðŸ“‹ Found {len(urls)} URLs to process\n")
        
        all_stats = []
        successful = 0
        failed = 0
        total_chunks = 0
        failed_urls = []
        
        for url in tqdm(urls, desc='Processing URLs', unit='url'):
            stats = process_url(url, chunk_size, overlap, strict=strict)
            all_stats.append(stats)
            
            if stats['success']:
                successful += 1
                total_chunks += stats['uploaded_chunks']
            else:
                failed += 1
                failed_urls.append(url)
                logging.error(f"âŒ Failed: {', '.join(stats['errors'])}")
        
        # Save failed URLs to file
        failed_file = None
        if failed_urls:
            failed_file = Path(filename).parent / "failed_uploaded_urls.txt"
            with open(failed_file, 'w', encoding='utf-8') as f:
                for url in failed_urls:
                    f.write(f"{url}\n")
            logging.info(f"ðŸ’¾ Failed URLs saved to: {failed_file.absolute()}")
        
        # Final report
        print(f"\n{'='*80}")
        print("âœ¨ PROCESSING COMPLETE")
        print(f"{'='*80}")
        print(f"âœ… Successful: {successful}/{len(urls)} URLs")
        print(f"âŒ Failed: {failed}/{len(urls)} URLs")
        if failed_file:
            print(f"ðŸ“ Failed URLs saved to: {failed_file.absolute()}")
        print(f"ðŸ“¦ Total chunks uploaded: {total_chunks}")
        
        # Detailed stats
        if successful > 0:
            avg_chunks = total_chunks / successful
            print(f"ðŸ“Š Average chunks per document: {avg_chunks:.1f}")
        
        print(f"{'='*80}\n")
        
    except FileNotFoundError:
        logging.error(f"âŒ File not found: {filename}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"âŒ Error: {e}")
        sys.exit(1)


def check_ollama_connection() -> bool:
    """Check if Ollama server is reachable by hitting /api/tags endpoint."""
    try:
        import os
        import requests
        ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
        r = requests.get(f"{ollama_url}/api/tags", timeout=5)
        r.raise_for_status()
        return True
    except Exception as e:
        logging.error(f"Ollama connection failed: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(
        description='Add URLs to Qdrant with semantic chunking',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s urls.txt
  %(prog)s urls.txt --chunk-size 600 --overlap 60
  %(prog)s urls.txt --verbose

File format (urls.txt):
  https://example.com/doc1
  https://example.com/doc2
  # Comments are ignored
"""
    )

    parser.add_argument('urls_file', help='File containing URLs (one per line)')
    parser.add_argument(
        '--chunk-size',
        type=int,
        default=DEFAULT_CHUNK_SIZE,
        help=f'Target chunk size in tokens (default: {DEFAULT_CHUNK_SIZE})'
    )
    parser.add_argument(
        '--overlap',
        type=int,
        default=DEFAULT_OVERLAP,
        help=f'Overlap between chunks in tokens (default: {DEFAULT_OVERLAP})'
    )
    parser.add_argument(
        '--max-tokens',
        type=int,
        default=MAX_TOKENS,
        help=f'Maximum tokens per chunk (default: {MAX_TOKENS})'
    )
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )
    parser.add_argument(
        '--strict',
        action='store_true',
        help='Abort on any fallback or error'
    )

    args = parser.parse_args()

    # Validate arguments
    if args.chunk_size <= 0:
        parser.error('--chunk-size must be a positive integer')
    if args.overlap < 0:
        parser.error('--overlap must be a nonâ€‘negative integer')
    if args.max_tokens <= 0:
        parser.error('--max-tokens must be a positive integer')

    # Setup logging
    setup_logging(args.verbose)

    # Print configuration
    logging.info("ðŸš€ RAG Document Indexer with Semantic Chunking")
    logging.info(f"ðŸ“ URLs file: {args.urls_file}")
    logging.info(f"âœ‚ï¸  Chunk size: {args.chunk_size} tokens (overlap: {args.overlap})")
    logging.info(f"ðŸ¤– Embedding model: nomic-embed-text via Ollama")
    logging.info(f"ðŸ—„ï¸  Collection: documentation")

    # Check Qdrant connection
    if not check_qdrant_connection():
        logging.error("âŒ Cannot connect to Qdrant")
        sys.exit(1)
    logging.info("âœ… Qdrant connected\n")

    # Check Ollama connection
    if not check_ollama_connection():
        logging.error("âŒ Cannot connect to Ollama")
        sys.exit(1)
    logging.info("âœ… Ollama connected\n")

    # Process URLs
    process_urls_file(args.urls_file, args.chunk_size, args.overlap, strict=args.strict)


if __name__ == "__main__":
    main()
